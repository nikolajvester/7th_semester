{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikolajvester/7th_semester/blob/main/notebooks/M2-pol_tweets_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor -q\n",
        "\n",
        "# Installing Gensim and PyLDAvis\n",
        "!pip install -qq -U gensim\n",
        "!pip install -qq pyLDAvis"
      ],
      "metadata": {
        "id": "a7ClxNmEW8-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530c9dcd-8c96-4726-fbfc-7c41859553e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 24.1 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 6.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# explainability (why did the model say it's hate speech)\n",
        "!pip install eli5"
      ],
      "metadata": {
        "id": "Yfx3zk8hOcRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e415fa8-ea07-4485-f0c2-ff6b336bdf86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting eli5\n",
            "  Downloading eli5-0.13.0.tar.gz (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>17.1.0 in /usr/local/lib/python3.7/dist-packages (from eli5) (22.1.0)\n",
            "Collecting jinja2>=3.0.0\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 59.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from eli5) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from eli5) (1.7.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from eli5) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from eli5) (1.0.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from eli5) (0.10.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from eli5) (0.8.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from jinja2>=3.0.0->eli5) (2.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->eli5) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->eli5) (1.2.0)\n",
            "Building wheels for collected packages: eli5\n",
            "  Building wheel for eli5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for eli5: filename=eli5-0.13.0-py2.py3-none-any.whl size=107748 sha256=c31b9a40a895e284a27c56567775361f5e62f9ae4dd8e39fb6c23a4353903d3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/3c/96/3ead31a8e6c20fc0f1a707fde2e05d49a80b1b4b30096573be\n",
            "Successfully built eli5\n",
            "Installing collected packages: jinja2, eli5\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\n",
            "Successfully installed eli5-0.13.0 jinja2-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMGBKNqTRcNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace53bf0-f5e7-4442-9605-803802f4ad83"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import preprocessor as prepro # twitter prepro\n",
        "import tqdm #progress bar\n",
        "\n",
        "import spacy #spacy for quick language prepro\n",
        "nlp = spacy.load('en_core_web_sm') #instantiating English module\n",
        "\n",
        "# sampling, splitting\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# loading ML libraries\n",
        "from sklearn.pipeline import make_pipeline #pipeline creation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #transforms text to sparse matrix\n",
        "from sklearn.linear_model import LogisticRegression #Logit model\n",
        "from sklearn.metrics import classification_report #that's self explanatory\n",
        "from sklearn.decomposition import TruncatedSVD #dimensionality reduction\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import altair as alt #viz\n",
        "\n",
        "#explainability\n",
        "import eli5\n",
        "from eli5.lime import TextExplainer\n",
        "\n",
        "# topic modeling\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary # Import the dictionary builder\n",
        "from gensim.models import LdaMulticore # we'll use the faster multicore version of LDA\n",
        "\n",
        "# Import pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "%matplotlib inline\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n",
            "/usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Mapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsQWaTU41ksj"
      },
      "source": [
        "# prepro settings\n",
        "prepro.set_options(prepro.OPT.URL, prepro.OPT.NUMBER, prepro.OPT.RESERVED, prepro.OPT.MENTION, prepro.OPT.SMILEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pol_tweets = pd.read_json('https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/pol_tweets.gz')"
      ],
      "metadata": {
        "id": "au8JEktn4ZSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pol_tweets.iloc[-1]['text'])\n",
        "print(pol_tweets.iloc[-1]['labels'])"
      ],
      "metadata": {
        "id": "q4b0E9St4gl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write everything into one function that can be re-used later\n",
        "def text_prepro(texts):\n",
        "  \"\"\"\n",
        "  takes in a pandas series (1 column of a DF)\n",
        "  removes twitter stuff\n",
        "  lowercases, normalizes text\n",
        "  \"\"\"\n",
        "  texts_clean = texts.map(lambda t: prepro.clean(t))\n",
        "  texts_clean = texts_clean.str.replace('#','')\n",
        "\n",
        "  clean_container = []\n",
        "\n",
        "  pbar = tqdm.tqdm(total=len(texts_clean),position=0, leave=True)\n",
        "\n",
        "  for text in nlp.pipe(texts_clean, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "    txt = [token.lemma_.lower() for token in text \n",
        "          if token.is_alpha \n",
        "          and not token.is_stop \n",
        "          and not token.is_punct]\n",
        "\n",
        "    clean_container.append(\" \".join(txt))\n",
        "    pbar.update(1)\n",
        "  \n",
        "  return clean_container"
      ],
      "metadata": {
        "id": "-8_LooEbCkNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply all prepro-pipeline to texts\n",
        "pol_tweets['text_clean'] = text_prepro(pol_tweets['text'])"
      ],
      "metadata": {
        "id": "qPLMu4XPCj3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pol_tweets.labels.value_counts().reset_index()"
      ],
      "metadata": {
        "id": "iUuwD1PiEQdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fixing sample imbalance\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "data_df_res, y_res = rus.fit_resample(pol_tweets, pol_tweets['labels'])"
      ],
      "metadata": {
        "id": "ncSGL6iNEQLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into the Training set and Test set (since we have a new output variable)\n",
        "X_train, X_test, y_train, y_test = train_test_split(pol_tweets['text_clean'], pol_tweets['labels'], test_size = 0.4, stratify=pol_tweets['labels'], random_state = 42)"
      ],
      "metadata": {
        "id": "9M5Q4jJxEw3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate models and \"bundle up as pipeline\"\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "cls = LogisticRegression()\n",
        "\n",
        "pipe = make_pipeline(tfidf, cls)"
      ],
      "metadata": {
        "id": "hZl3ys9WFKTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(X_train,y_train) # fit model"
      ],
      "metadata": {
        "id": "okQT5dM3E1C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model performance on training set\n",
        "\n",
        "y_eval = pipe.predict(X_train)\n",
        "report = classification_report(y_train, y_eval)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "6ne8wlHfHfyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall weights (works only for linear models)\n",
        "eli5.show_weights(pipe, top=20, target_names=['Rep.','Dem.'])"
      ],
      "metadata": {
        "id": "FSnVKv5dHfiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pipe.predict(X_test)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "j82SH-UgHfTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4U2fP2V3JJZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debate_tweets = pd.read_json('https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/pres_debate_2020.gz')"
      ],
      "metadata": {
        "id": "8Sj-kA_O4pVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debate_tweets"
      ],
      "metadata": {
        "id": "lfndElce47Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply all prepro-pipeline to texts\n",
        "debate_tweets['text_clean'] = text_prepro(debate_tweets['tweet'])"
      ],
      "metadata": {
        "id": "muEnWfkf48dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debate_tweets['class_pred'] = pipe.predict(debate_tweets['text_clean'])"
      ],
      "metadata": {
        "id": "EiO5WwAXJrei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debate_tweets['dem_proba'] = pipe.predict_proba(debate_tweets['text_clean'])[:,1]"
      ],
      "metadata": {
        "id": "QXtvH4VyKE9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debate_tweets.sort_values(['dem_proba'], ascending=True)['tweet'][:10]"
      ],
      "metadata": {
        "id": "pj2_bXDuKyaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debate_tweets.sort_values(['dem_proba'], ascending=False)"
      ],
      "metadata": {
        "id": "-t-bQVrvLY48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic modellng - what do dem/rep tweets say?\n",
        "\n",
        "you can try out the tweetopic library (it's rather new, developed by people from Århus Uni. and I'm not sure about it so far: https://centre-for-humanities-computing.github.io/tweetopic/using_tweetopic.pipeline.html)"
      ],
      "metadata": {
        "id": "ylPexHvr9tZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "debate_tweets['dem_proba'].hist()"
      ],
      "metadata": {
        "id": "X2_dary8_Li7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rep_tweets = debate_tweets[debate_tweets['dem_proba']<=0.2]"
      ],
      "metadata": {
        "id": "2FUBUY6d9saF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess texts (we need tokens)\n",
        "tokens = []\n",
        "\n",
        "for text in nlp.pipe(rep_tweets['text_clean'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in text \n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
        "              and not token.is_stop\n",
        "              and not token.is_punct] \n",
        "  tokens.append(proj_tok)"
      ],
      "metadata": {
        "id": "4Qr4hqd5__80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rep_tweets['tokens'] = tokens"
      ],
      "metadata": {
        "id": "PrbT0GFID7nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(rep_tweets['tokens'])\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 1000 words\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in rep_tweets['tokens']]"
      ],
      "metadata": {
        "id": "X_sFiQXgED34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=5, workers = 4, passes=10)"
      ],
      "metadata": {
        "id": "Q28WQIKLELFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try to visualize\n",
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ],
      "metadata": {
        "id": "oGUt2bf7ENuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Let's Visualize\n",
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "b32JzALaEPbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tunint model N topics**\n",
        "\n",
        "We can evaluate coherence from gensim.\n",
        "Other measures are available within the Octis libary: https://github.com/mind-Lab/octis\n",
        "\n",
        "Also consider:\n",
        "\n",
        "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n"
      ],
      "metadata": {
        "id": "A1IXLorTHPUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel"
      ],
      "metadata": {
        "id": "_NS83s3ZESlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=rep_tweets['tokens'], dictionary=dictionary, coherence='u_mass')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "id": "eBfpgDpEFBet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### other models"
      ],
      "metadata": {
        "id": "zMz7yp3JH1tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LsiModel, TfidfModel\n",
        "from gensim.matutils import corpus2dense"
      ],
      "metadata": {
        "id": "qrhyWmLGFfVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfModel(corpus)"
      ],
      "metadata": {
        "id": "8qLwBmDyIFl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_tfidf = tfidf[corpus]"
      ],
      "metadata": {
        "id": "BSPCZLtqIXyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lsi = LsiModel(corpus_tfidf, num_topics=10, id2word=dictionary)"
      ],
      "metadata": {
        "id": "8GvwTOAZIhLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lsi.print_topics()"
      ],
      "metadata": {
        "id": "wwQOlYelJGQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_lsi = lsi[corpus_tfidf]"
      ],
      "metadata": {
        "id": "W9vJVc2dIrfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lsi_matrix = corpus2dense(corpus_lsi, num_terms = 10)"
      ],
      "metadata": {
        "id": "tSkgsEB7IxGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lsi_matrix.T[1]"
      ],
      "metadata": {
        "id": "A18Na5E0Iyg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_lsi[1]"
      ],
      "metadata": {
        "id": "poFY3cgXKqBJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}